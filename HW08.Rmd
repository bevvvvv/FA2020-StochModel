---
title: "Homework 08 - STAT416"
author: "Joseph Sepich (jps6444)"
date: "11/03/2020"
output:
  pdf_document:
    number_sections: false
---

# Chapter 4 Problem 70

## Part a

In this Markov chain the state is the number of $m$ black balls in the first urn. This means that there are $m + 1$ total states: $0,1,2...,m$.

The state $i$ can only transition from $i$ to either $i-1$, $i$, or $i+1$. Losing one black ball means pulling a black ball from urn 1 and a white ball from urn 2. This occurs with probability $P_{i,i-1}=\frac{i}{m}\frac{i}{m} = \frac{i^2}{m^2}$. Gaining one black ball means pulling a white ball from urn 1 and a black ball from urn 2. This occurs with probability $P_{i,i+1}=\frac{m-i}{m}\frac{m-i}{m} = \frac{(m-1)^2}{m^2}$. Maintaining the same number of black balls in urn 1 involves pulling the same color from both urns. This happens with probability $P_{i,i} = \frac{i}{m}\frac{m-i}{m} + \frac{m-i}{m}\frac{i}{m} = \frac{2i(m-i)}{m^2}$.

## Part b

I think that the limiting probabilities of this chain would be the possible ways to have i black balls in urn 1 out of the possible ways to divide the balls.

## Part c

\[\pi_i = \pi_{i-1}\frac{(m-1)^2}{m^2} + \pi_i\frac{2i(m-i)}{m^2} + \pi_{i+1}\frac{i^2}{m^2}\]

This chain is isolated to going up or down in the current state and does not involve loops, so it is reversible. If that is true then:

\[\pi_iP_{i,i+1} = \pi_{i+1}P_{i+1,i}\]
\[\frac{\pi_{i+1}}{\pi_i} = \frac{P_{i,i+1}}{P_{i+1,i}} = (\frac{m-i}{i+1})^2\]

If this is true we can then go back and prove $\pi_i$ by induction.

\[\frac{\pi_1}{\pi_0} = m^2\]
\[\frac{\pi_2}{\pi_0} =(\frac{m}{1}\frac{m-1}{2})^2=\binom{m}{2}^2\]

Continuing this by induction we get

\[\frac{\pi_i}{\pi_0} = \binom{m}{i}^2\]

From this result we then know how often $\pi_i$ will happen. We just add in the denominator, which includes all possible options:


\[\pi_i = \frac{\binom{m}{i}^2}{\binom{m}{0}^2+\binom{m}{1}^2+...+\binom{m}{m}^2}\]


# Chapter 4 Problem 72

We know that in a time reversible Markov chain the following rate between i and j is equal to the rate between j and i and j and k:

\[\pi_iP_{ij} = \pi_jP_{ji}\]
\[\pi_jP_{jk} = \pi_kP_{kj}\]

The transition rate of i to j to k would be represented as:

\[\pi_iP_{ij}P_{jk}\]

We are trying to prove that 

\[\pi_iP_{ij}P_{jk} = \pi_kP_{kj}P_{ji}\]


We can substitute using the first two equations:

\[\pi_iP_{ij}P_{jk} = \pi_iP_{ij}\pi_kP_{kj}/\pi_j = \pi_jP_{ji}\pi_kP_{kj}/\pi_j = \pi_kP_{kj}P_{ji}\]

Therefore from substitution of the adjacent rates of transition we can conclude that the rate of transition from i to j to k is the same as from k to j to i.


# Chapter 4 Problem 73

## Part a

There are k possible states in this Markov chain. State i as stated in the problem corresponds to player i winning the game. There are two cases when a game is played. The current winner/state could win or the challenger (randomly selected palyer) could win. The transition probability if player i wins is $P_{ii} = \frac{v_i}{(k-1)(v_i+v_j)}$, which is dependent on challenger j. The transition probability if a challenger player j wins is $P_{ij} = \frac{v_j}{(k-1)(v_i+v_j)}$.

## Part b

The stationary probability involes two components. Player j is challenge as the winner and remains the winner, or player j challenges player i and becomes the winner.

\[\pi_j = \pi_j\Sigma_i\frac{v_j}{(v_i+v_j)} + (1-\pi_j)\Sigma_i\frac{v_j}{(v_i+v_j)}\]
\[\pi_j = \Sigma_i\frac{v_j}{(v_i+v_j)}(\pi_j + (1-\pi_j))\]
\[\pi_j = \Sigma_i\frac{v_j}{(v_i+v_j)}\]
\[\Sigma_j\pi_j = 1\]

## Part c

\[Q_{ij} = P_{ij} = \frac{\pi_jP_{ji}}{\pi_i}\]
\[\frac{v_j}{(k-1)(v_i+v_j)} = \frac{\pi_j\frac{v_i}{(k-1)(v_i+v_j)}}{\pi_i}\]
\[v_j = \frac{v_i\pi_j}{\pi_i}\]
\[\frac{v_j}{v_i} = \frac{\Sigma_i\frac{v_j}{(v_i+v_j)}}{\Sigma_i\frac{v_i}{(v_i+v_j)}}\]
\[1 = 1\]

## Part d

The proportion of games won by player j is the stationary probability of player j.

\[\pi_j = \Sigma_i\frac{v_j}{(v_i+v_j)}\]

## Part e

The proportion of games involving player j is the stationary probability, plus when j is picked and loses.

\[\pi_j + (1-\pi_j)P_{ij}\]
\[\Sigma_i\frac{v_j}{(v_i+v_j)} + (1 - \Sigma_i\frac{v_j}{(v_i+v_j)})\Sigma_i\frac{v_i}{(v_i+v_j)}\]

# Chapter 5 Problem 1

\[T \text{~} Exp(\lambda = 2)\]

## Part a

\[P(T > \frac12) = 1 - F(\frac12) = 1 - 1 + e^{-1} = e^{-1}\]

## Part b

Here we can apply the memoryless property:

\[P(T > 12 + \frac12 | T > 12) = P(T>\frac12) = e^{-1}\]

# Chapter 5 Problem 4

## Part a

If the service time of each clerk is exactly 10 minutes, then the probability A is still in the post office after B and C have left is zero. In this case A and B would enter, be served in ten minutes and leave, then C would be served for 10 minutes and leave.

## Part b

A could would only still be in the post office after B and C are service if the value of service A is $i=3$. The sum of B and C would have to be 2.

\[P(A > B + C) = \frac13(\frac13\frac13) = \frac{1}{27}\]

## Part c

Service times are Exponentially distributed with parameter $\mu$. Recall that the sum of exponentially distributed random variables is a 

\[P(A > B + C) = \int_0^\infty P(A>B+C|C=x)f_c(x)dx = \int_0^\infty P(A>B+x)\mu e^{-\mu x}dx\]
\[P(A > B + C) = \int_0^\infty\frac12e^{-\mu x}\mu e^{-\mu x}dx=\frac{\mu}{2}\int_0^\infty e^{-2\mu x}dx = \frac{\mu}{2}(\frac{-e^{-2\mu x}}{2\mu})|_0^\infty = \frac14\]

# Chapter 5 Problem 12

## Part a

\[P(X_1 < X_2< X_3) =  P(X_1 <X_2) P(X_2<X_3) = \frac{\lambda_1\lambda_2}{(\lambda_1+\lambda_2)(\lambda_2+\lambda_3)}\]

# Chapter 5 Problem 14

## Part a

We want to find the probability that A arrives before and departs after B. This implies that the arrival time for A is smaller than B, but the sum of arrival time and time spent for A is greater than B.

\[P(A<B)P(A+S_A > B+S_B| B>A,S_A>B) = P(A<B)P(B < S_A)P(S_A < S_B) = \frac{\lambda_A\lambda_b\mu_A}{(\lambda_A+\lambda_B)(\lambda_B+\mu_A)(\mu_A+\mu_B)}\]

# Chapter 5 Part 19

\[E[R_A] = E[R_A|W=A] + E[R_A|W=B] = E[R_A|W=A] + 0 = E[R_A|W=A] \]
\[E[R_A|W=A] = Re^{-\alpha t}P(A<B) = \frac{\lambda_ARe^{-\alpha t}}{\lambda_A+\lambda_B}\]

We expect A to earn $\frac{\lambda_ARe^{-\alpha t}}{\lambda_A+\lambda_B}$.

# Problem A

Recall that a Markov chain is reversible if $Q_{ij} = P_{ij} = \pi_jP_{ji}/\pi_i$. We had calculated the stationary probabilities.


```{r}
pi_0 <- c(-0.3,0.2,1)
pi_1 <- c(0.2,-0.4,1)
pi_2 <- c(0.1,0.4,1)
A <- cbind(pi_0, pi_1, pi_2)
b <- c(0,0,1)
solve(A,b)
```

We can see if this equality holds:

```{r}
stationary <- solve(A,b)
P <- matrix(c(0.7,0.2,0.1,0.2,0.6,0.4,0.1,0.2,0.5), nrow=3)
for (i in 1:3) {
  for (j in 1:3) {
    #print(P[i,j])
    equivalent <- stationary[j]*P[j,i]/stationary[i]
    print(equivalent == P[i,j])
  }
}
```

We can see that the Markov chain is not in fact time reversible. An example is going from state 1 to state 2.
