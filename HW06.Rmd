---
title: "Homework 06 - STAT416"
author: "Joseph Sepich (jps6444)"
date: "10/20/2020"
output:
  pdf_document:
    number_sections: false
---

# Chapter 4 Problem 17

Here we look at the infinite random walk in Example 4.19 from the book. The random variable $Y_i$ is an indicator of whether we went to the state 1 higher than the current state or not: $Y_i = \{1, -1\}$. Suppose $p = P(Y_i=1) > \frac12$. We know that the state at time $n$ can be written as $\Sigma_{i=1}^nY_i$. The expected value of $Y_i$ is

\[E[Y_i] = p + (p-1) = 2p-1\]

When $p= \frac12$ this expected value is $2*\frac12-1 = 0$, so when $p > \frac12$ the expected value is $1 > E[Y_i] > 0$ (equal to $2p-1$). We are looking at the value $\Sigma_{i=1}^nY_i = n\overline{Y_n}$

Recall the strong law of large numbers which states as $n \rightarrow \infty$, then $\overline{X_n} \rightarrow \mu$.

Similarly we can say as $n \rightarrow \infty$ then $\overline{Y_n} \rightarrow 2p-1$, so $n\overline{Y_n} = \Sigma_{i=1}^nY_i \rightarrow \infty$, since we have infinity multiplied by a constant we get infinity. Since we just concluded that the state at time n will go to $\infty$ if $p > \frac12$, then we can say 0 is only visited finitely often, and therefore must be transient. Since transient is a class property and there is only one class the whole chain must be transient.

Note that we can use similar logic when $p < \frac12$. Here we merely get a negative constant $0 > E[Y_i] > -1$. Therefore this proof holds for $p \neq \frac12$.

# Chapter 4 Problem 18

Probability transition matrix below includes the two states coin 1 and coin 2. Note that this Markov Chain is both recurrent and irreducible. There is a single class and we know that it is recurrent, because the expected value that a tail eventually occurs follows from the Geometric distribution, which is a finite value.

$$
P =\begin{bmatrix}
0.6 & 0.4\\
0.5 & 0.5
\end{bmatrix}
$$

## Part a

We want to find the proportion of flips that uses coin 1, which can be denoted as $\pi_1$.

\[\pi_1 = 0.6\pi_1 + 0.5\pi_2\]
\[\pi_2 = 0.4\pi_1 + 0.5\pi_2\]
\[\pi_1 + \pi_2 = 1\]

\[\pi_1 = \frac{0.5}{1 + 0.5-0.6} = \frac{0.5}{0.9} = \frac59\]

## Part b

We want to find the probability that we use coin 2 at time 5 given that we started with coin 1: $P(X_5=2 |X_1=1)$. To find this we can use P^4 combined with transitions to coin 2.

```{r}
P <- matrix(c(0.6,0.5,0.4,0.5), nrow=2, ncol=2)
P_5 <- P %*% P %*% P %*% P %*% P 
P_5
```

So we get:

\[P(X_5=2 |X_1=1) = P(X_5=2 |X_4=1)P(X_4=1 |X_1=1) + P(X_5=2 |X_4=2)P(X_4=2 |X_1=1)\]

\[P^5_{12} = P_{12}P^4_{11}+P_{22}P^4_{12} = 0.4\frac59+0.5\frac49\]

\[P^5_{12} = \frac49\]

## Part c

We want to know the proportion of times the flips land on heads. This is simply the given weight times the proportions we found before:

\[\pi_H = 0.6\pi_1+0.5\pi_2 = 0.6\frac59 + 0.5\frac49\]
\[\pi_H = \frac59\]

# Chapter 4 Problem 21

Transitional Probability $P_{i_j}$:

\[P_{i_j} = 1-1+3\alpha / 3 = \alpha; j \neq i\]
\[P_{i,i} = 1-3\alpha\]

## Part a

Show that $P^n_{1,1} = \frac14 + \frac34(1-4\alpha)^n$. To do this let's construct our transition probability matrix:

$$
P^1 =\begin{bmatrix}
1-3\alpha & \alpha & \alpha & \alpha\\
\alpha & 1-3\alpha & \alpha & \alpha\\
\alpha & \alpha & 1-3\alpha & \alpha\\
\alpha & \alpha & \alpha & 1-3\alpha
\end{bmatrix}
$$

Let's get $P^2_{1,1}$ first. All the diagonal entries of the output are the same, since the matrix is symmetric:

\[P^2_{1,1} = (1-3\alpha)^2+3\alpha^2 = 1-6\alpha+12\alpha^2 = \frac14 +\frac34-6\alpha + 12 \alpha^2 = \frac14 + \frac34(1-8\alpha+16\alpha^2) = \frac14 + \frac34(1-4\alpha)^2\]

Since all the non-diagonal entries are the same and the matrix is symmetric the pattern will continue so $P^2_{1,1} = \frac14 + \frac34(1-4\alpha)^n$.

# Chapter 4 Problem 23

## Part a

We want to find the expected total number of storms in the next two years (years 1 and 2). We know that the expected number of storms depends on whether the year is in the good weather state or bad weather state where the expected number of storms are 1 and 3 respectively.

\[E[N_1+N_2] = E[N_1] + E[N_2] = 1P_{GG}+3P_{GB} + 1P^2_{GG}+ 3P^2_{GB}\]

Suppose row/column 1 is the state good year and row/column 2 of the probability transition matrix is teh state bad year:

$$
P^1 =\begin{bmatrix}
\frac12 & \frac12\\
\frac13 & \frac23
\end{bmatrix}
$$

$$
P^2 =\begin{bmatrix}
\frac12 & \frac12\\
\frac13 & \frac23
\end{bmatrix}^2 = \begin{bmatrix}
\frac3{12} & \frac7{12}\\
\frac7{18} & \frac{11}{18}
\end{bmatrix}
$$

\[E[N_1+N_2] = \frac12 + \frac32 + \frac3{12} + \frac{21}{12} = \frac{48}{12} = 4\]

We expect there to be 4 storms in the next two years.

## Part b

We want to find the probability that there are no storms in year 3. There are two cases we must consider: no storms in year 3, which is a bad year or no storms in year 3, which is a good year.

\[P(N = 0) = P_G(0)P^3_{GG} + P_B(0)P^3_{GB}\]

$$
P^3 =\begin{bmatrix}
\frac3{12} & \frac7{12}\\
\frac7{18} & \frac{11}{18}
\end{bmatrix}\begin{bmatrix}
\frac12 & \frac12\\
\frac13 & \frac23
\end{bmatrix} = \begin{bmatrix}
\frac{23}{72} & \frac{37}{72}\\
\frac{43}{108} & \frac{65}{108}
\end{bmatrix}
$$

\[P(N = 0) = e^{-1}\frac{23}{72} + e^{-3}\frac{37}{72} \approx 0.1431\]

The probability of having no storms in year 3 is approximately **14.31%**.

## Part c

We want to find the long-run average number of storms per year ($N_\pi$). To do this we want to find the proportion of time we spend in a good year and bad year respectively and multiply by their average (1 and 3):

\[N_\pi = 1\pi_G+3\pi_B\]
\[\pi_G = \frac12\pi_G + \frac13\pi_B\]
\[\pi_B = \frac12\pi_G + \frac23\pi_B\]
\[\pi_G + \pi_B = 1\]
\[\pi_G = \frac{\frac13}{1 + \frac13 - \frac12} = \frac{\frac13}{\frac56} = \frac25\]
\[\pi_B = 1 - \pi_G = \frac35\]

\[N_\pi = \frac25+3\frac35 = \frac{11}5\]

We expect there to be $\frac{11}5$ storms per year.

# Chapter 4 Problem 33

Probability transition matrix of which player serves.

$$
P^1 =\begin{bmatrix}
p & 1-p\\
q & 1-q
\end{bmatrix}
$$

We want to find the proportion of points that are won by player 1, which is the proportion of time player 1 serves: $\pi_1$.

\[\pi_1 = p\pi_1 + q\pi_2\]
\[\pi_2 = (1-p)\pi_1 + (1-q)\pi_2\]
\[\pi_1 + \pi_2 = 1\]
\[\pi_1 = \frac{q}{1+q-p} = \frac{q}{q+(1-p)}\]
\[\pi_2 = \frac{1-q}{q+(1-p)}\]

The proportion of time that player 1 serves/wins a point is $\pi_1 = \frac{q}{q+(1-p)}$.

# Chapter 4 Problem 36

$$
P^1 =\begin{bmatrix}
0.4 & 0.6\\
0.2 & 0.8
\end{bmatrix}
$$

## Part a

We want to determine the probability that a good message is sent on the second day.

\[P(\text{"good"},n=1) = p_iP^n_{0i}= p_0P_{00} + p_1P_{01} = 0.4p_0 + 0.6p_1\]

## Part b

We want to determine the rpobability that a good message is sent onf the fifth day.

\[P(\text{"good"},n=5) = p_iP^n_{0i} = p_0P^4_{00} + p_1P^4_{01}\]

Let's calculate $P^4$:

```{r}
P <- matrix(c(0.4,0.2,0.6,0.8),nrow=2)
P %*% P %*% P %*% P
```

\[P(\text{"good"},n=5) = 0.2512p_0 + 0.7488p_1\]

## Part c

We want to know the proportion of good messages in the long run. This is the probability $p_i$ times the proportion of time spent in each state.

\[\pi_0 = 0.4\pi_0 + 0.2\pi_1\]
\[\pi_1 = 0.6\pi_0 + 0.8\pi_1\]
\[\pi_0 + \pi_1 = 1\]
\[\pi_0 = \frac{0.2}{1+0.2-0.4} = \frac{0.2}{0.8} = \frac14\]
\[\pi_1 = 1 - \pi_0 = \frac34\]

This gives us the proportion of good messages:

\[\pi_G = p_0\pi_0 + p_1\pi_1 = \frac{p_0}{4} + \frac{3p_1}{4}\]

## Part d

The proposition for a new Markov Chain gives two states: a good message is sent or a bad message is sent. This is not a Markov Chain. Since the probaility of seeing a good or bad message depends on the state in the first Markov Chain specified, then we break the Markov property, because the probability of say going from a good message state to a good message state depends more on just that it started in a good message state, it also depends on the underlying state from the first Markov Process.





