---
title: "Homework 06 - STAT416"
author: "Joseph Sepich (jps6444)"
date: "10/20/2020"
output:
  pdf_document:
    number_sections: false
---

# Chapter 4 Problem 17

Here we look at the infinite random walk in Example 4.19 from the book. The random variable $Y_i$ is an indicator of whether we went to the state 1 higher than the current state or not: $Y_i = \{1, -1\}$. Suppose $p = P(Y_i=1) > \frac12$. We know that the state at time $n$ can be written as $\Sigma_{i=1}^nY_i$. The expected value of $Y_i$ is

\[E[Y_i] = p + (p-1) = 2p-1\]

When $p= \frac12$ this expected value is $2*\frac12-1 = 0$, so when $p > \frac12$ the expected value is $1 > E[Y_i] > 0$ (equal to $2p-1$). We are looking at the value $\Sigma_{i=1}^nY_i = n\overline{Y_n}$

Recall the strong law of large numbers which states as $n \rightarrow \infty$, then $\overline{X_n} \rightarrow \mu$.

Similarly we can say as $n \rightarrow \infty$ then $\overline{Y_n} \rightarrow 2p-1$, so $n\overline{Y_n} = \Sigma_{i=1}^nY_i \rightarrow \infty$, since we have infinity multiplied by a constant we get infinity. Since we just concluded that the state at time n will go to $\infty$ if $p > \frac12$, then we can say 0 is only visited finitely often, and therefore must be transient. Since transient is a class property and there is only one class the whole chain must be transient.

Note that we can use similar logic when $p < \frac12$. Here we merely get a negative constant $0 > E[Y_i] > -1$. Therefore this proof holds for $p \neq \frac12$.

# Chapter 4 Problem 18

Probability transition matrix below includes the two states coin 1 and coin 2. Note that this Markov Chain is both recurrent and irreducible. There is a single class and we know that it is recurrent, because the expected value that a tail eventually occurs follows from the Geometric distribution, which is a finite value.

$$
P =\begin{bmatrix}
0.6 & 0.4\\
0.5 & 0.5
\end{bmatrix}
$$

## Part a

We want to find the proportion of flips that uses coin 1, which can be denoted as $\pi_1$.

\[\pi_1 = 0.6\pi_1 + 0.5\pi_2\]
\[\pi_2 = 0.4\pi_1 + 0.5\pi_2\]
\[\pi_1 + \pi_2 = 1\]

\[\pi_1 = \frac{0.5}{1 + 0.5-0.6} = \frac{0.5}{0.9} = \frac59\]

## Part b

We want to find the probability that we use coin 2 at time 5 given that we started with coin 1: $P(X_5=2 |X_1=1)$. To find this we can use P^4 combined with transitions to coin 2.

```{r}
P <- matrix(c(0.6,0.5,0.4,0.5), nrow=2, ncol=2)
P_5 <- P %*% P %*% P %*% P %*% P 
P_5
```

So we get:

\[P(X_5=2 |X_1=1) = P(X_5=2 |X_4=1)P(X_4=1 |X_1=1) + P(X_5=2 |X_4=2)P(X_4=2 |X_1=1)\]

\[P^5_{12} = P_{12}P^4_{11}+P_{22}P^4_{12} = 0.4\frac59+0.5\frac49\]

\[P^5_{12} = \frac49\]

## Part c

We want to know the proportion of times the flips land on heads. This is simply the given weight times the proportions we found before:

\[\pi_H = 0.6\pi_1+0.5\pi_2 = 0.6\frac59 + 0.5\frac49\]
\[\pi_H = \frac59\]

# Chapter 4 Problem 21

Transitional Probability $P_{i_j}$:

\[P_{i_j} = 1-1+3\alpha / 3 = \alpha; j \neq i\]
\[P_{i,i} = 1-3\alpha\]

## Part a

Show that $P^n_{1,1} = \frac14 + \frac34(1-4\alpha)^n$. To do this let's construct our transition probability matrix:

$$
P^1 =\begin{bmatrix}
1-3\alpha & \alpha & \alpha & \alpha\\
\alpha & 1-3\alpha & \alpha & \alpha\\
\alpha & \alpha & 1-3\alpha & \alpha\\
\alpha & \alpha & \alpha & 1-3\alpha
\end{bmatrix}
$$

Let's get $P^2_{1,1}$ first. All the diagonal entries of the output are the same, since the matrix is symmetric:

\[P^2_{1,1} = (1-3\alpha)^2+3\alpha^2 = 1-6\alpha+12\alpha^2 = \frac14 +\frac34-6\alpha + 12 \alpha^2 = \frac14 + \frac34(1-8\alpha+16\alpha^2) = \frac14 + \frac34(1-4\alpha)^2\]

Since all the non-diagonal entries are the same and the matrix is symmetric the pattern will continue so $P^2_{1,1} = \frac14 + \frac34(1-4\alpha)^n$.

# Chapter 4 Problem 23

## Part a

## Part b

## Part c

# Chapter 4 Problem 33

# Chapter 4 Problem 36








