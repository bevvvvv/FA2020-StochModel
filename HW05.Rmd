---
title: "Homework 05 - STAT416"
author: "Joseph Sepich (jps6444)"
date: "10/13/2020"
output:
  pdf_document:
    number_sections: false
---

# Chapter 4 Problem 5

We are given both the initial probability of starting at state $i$ along with the 1 step transition probability matrix. We can use both of these to determine $P(X_3) = \alpha P^3$.

$$
\alpha^1 = \alpha^0 P=\begin{bmatrix}\frac14 & \frac14 & \frac12\end{bmatrix}\begin{bmatrix}
\frac12 & \frac 13 & \frac16 \\
0 & \frac13 & \frac23 \\
\frac12 & 0 & \frac12
\end{bmatrix} = \begin{bmatrix}
\frac38 & \frac16 & \frac{11}{24}
\end{bmatrix}
$$

$$
\alpha^2 = \alpha^1 P=\begin{bmatrix}\frac38 & \frac16 & \frac{11}{24}\end{bmatrix}\begin{bmatrix}
\frac12 & \frac 13 & \frac16 \\
0 & \frac13 & \frac23 \\
\frac12 & 0 & \frac12
\end{bmatrix} = \begin{bmatrix}
\frac5{12} & \frac{13}{72} & \frac{29}{72}
\end{bmatrix}
$$

$$
\alpha^3 = \alpha^2 P=\begin{bmatrix}\frac5{12} & \frac{13}{72} & \frac{29}{72}\end{bmatrix}\begin{bmatrix}
\frac12 & \frac 13 & \frac16 \\
0 & \frac13 & \frac23 \\
\frac12 & 0 & \frac12
\end{bmatrix} = \begin{bmatrix}
\frac{59}{144} & \frac{43}{216} & \frac{169}{432}
\end{bmatrix}
$$

\[E[X_3] = 0 + \frac{43}{216} + \frac{169}{216} = \frac{212}{216} \approx 0.9815\]

# Chapter 4 Problem 8

## Part c

To find the probability that the second ball selected is red we need to know the probability of the state at time 2 (from which the ball will be selected). We use our initial probabilities combined with our transition probability matrix:

$$
\alpha^2 = \alpha^1 P=\begin{bmatrix}0 & 1 & 0\end{bmatrix}\begin{bmatrix}
0.5 & 0.5 & 0 \\
0.15 & 0.6 & 0.25 \\
0 & 0.3 & 0.7
\end{bmatrix} = \begin{bmatrix}
0.15 & 0.6 & 0.25
\end{bmatrix}
$$

So the probability of drawing a red ball second when there is 0 red balls is 0, 0.5 when there is 1 red ball, and 1 when there is 2 red balls. This gives us the probability $0 * 0.15 + 0.5 * 0.6 + 1 * 0.25 = 0.25 + 0.3 = 0.55$. The probability of selecting a red ball second is **55%**.

## Part d

Calculating the probability of selecting a red ball fourth follows the same logic as the previous part. We can pickup with $\alpha_2$.

$$
\alpha^3 = \alpha^2 P=\begin{bmatrix}0.15 & 0.6 & 0.25\end{bmatrix}\begin{bmatrix}
0.5 & 0.5 & 0 \\
0.15 & 0.6 & 0.25 \\
0 & 0.3 & 0.7
\end{bmatrix} = \begin{bmatrix}
0.165 & 0.51 & 0.325
\end{bmatrix}
$$
$$
\alpha^4 = \alpha^3 P=\begin{bmatrix}0.165 & 0.51 & 0.325\end{bmatrix}\begin{bmatrix}
0.5 & 0.5 & 0 \\
0.15 & 0.6 & 0.25 \\
0 & 0.3 & 0.7
\end{bmatrix} = \begin{bmatrix}
0.159 & 0.486 & 0.355
\end{bmatrix}
$$

This gives us the probability $0 * 0.159 + 0.5 * 0.486 + 1 * 0.355 = 0.243 + 0.355 = 0.598$. The probability of selecting a red ball fourth is **59.8%**. It makes sense that the likelihood of selecting red goes up with time as you are more likely to replace a red ball with a red ball while replace blue ball has equal probability for each.

# Chapter 4 Problem 10

The probability that we desire from Gary given our Markov chain $\{X_n,n \geq 0\}$ is $P(X_{i+k} \notin  \{G\} \text{for k} = 1,2,3|X_i = C)$. This is also the complement that is every in a glum mood: $1 - P(X_{i+k} \in  \{G\} \text{for k} = 1,2,3|X_i = C)$.  Here we will define our Markov chain $\{W_n, n\geq 0\}$. Its states are the states the same as $X_n$, but as soon as it enters the state Glum it stays there. For this chain we get the transition probability matrix:

$$
Q =\begin{bmatrix}
0.5 & 0.4 & 0.1 \\
0.3 & 0.4 & 0.3 \\
0 & 0 & 1
\end{bmatrix}
$$

All we need to do now is calculate $Q^3_{CG}$:

```{r}
Q <- matrix(c(0.5, 0.3, 0, 0.4, 0.4, 0, 0.1, 0.3, 1), nrow=3, ncol=3)
Q
```
```{r}
Q %*% Q %*% Q
```

The probability of Gary never being in a glum mood the following three days, given that he starts cheerful is 1 - 0.415 = **58.5%**.

# Chapter 4 Problem 14

For the first transition probability matrix $P_1$ the matrix is irreducible so it only has one class $\{0,1,2\}$, where the states are transient (the class as a whole is recurrent since it will always be in the class).

For the second transition probability matrix $P_2$ the matrix has a single class $\{0,1,2,3\}$ where states 2 and 3 are recurrent and states 0 and 1 are transient (class as a whole is recurrent).

For the third transition probability matrix $P_3$ the matrix has the states $\{0, 2\}$, $\{1\}$, $\{3,4\}$. The only transient class is $\{1\}$ (so the other classes are recurrent) and there are no recurrent states.

For the fourth transition probability matrix $P_4$ the matrix has the states $\{0, 1\}$, $\{2\}$, $\{3\}$ and $\{4\}$. Classes $\{3\}$ and $\{4\}$ are transient, while classes $\{0, 1\}$ and $\{2\}$ are recurrent. Note that the state 2 is also recurrent.

# Chapter 4 Problem 15

The conditional gives us two things to consider. The first is that state $j$ can be reached from state $i$. This gives us $P^m_{i,j} > 0$ and we know that state j can be reached in m steps. Our starting state is i and our ending states is j. As we know $P^m_{i,j}$ can be found via our forward probaility equation $P^m_{i,j} = \alpha_j^m = (\alpha_{m-1}P)_j > 0$. We know that this value must give us:

\[P_{ij}^m = \Sigma_{n<m}P_{n_0n_1}P_{n_1n_2}...P_{n_{m-2}n_{m-1}}P_{n_{m-1}n_m} > 0\]

We can follow one path (instead of all) from i to j then:

\[P_{n_0n_1}P_{n_1n_2}...P_{n_{m-2}n_{m-1}}P_{n_{m-1}n_m} > 0\]

Now we know that this path could get shorter if the state was the same for two of these timestamps (like if Gary in the previous problem stayed Cheerful for two days in a row). Let us denote this states $n_x = n_y$:

\[P_{n_0n_1}P_{n_1n_2}...P_{n_{x-1}n_{x}}P_{n_{x}n_{y+1}}...P_{n_{m-2}n_{m-1}}P_{n_{m-1}n_m} > 0\]

Since our shortest possible path would only stop at these states once (no duplicates) the number of transitions could then also be less than m.

# Chapter 4 Problem 16

First let use recall what it means to be recurrent. If state $i$ is recurrent, then with probability equal to 1 we know that the process will reenter state $i$. This means that eventually the process will reutrn to state $i$, but does not necessarily need to stay in state $i$. We also know that state $i$ and state $j$ do not communicate with each other, meaning that state $j$ is not accessible from state $i$. By definition of communicates and accessible we actually already know that $P_{ij}^n = 0$, since if state $j$ was accessible, then $P_{ij}^n > 0$, but it is not.

# Problem A

The transition probability matrix below denotes row 0 as the processing facility A, row 1 as supermarket B, and row 3 as supermarket C. The same mapping to the columns.

$$
P =\begin{bmatrix}
0 & \frac12 & \frac12 \\
\frac23 & 0 & \frac13 \\
\frac23 & \frac13 & 0
\end{bmatrix}
$$

We want to know $P^2_{Cj}$. To find this we simply need to compute the matrix $P^2$ via matrix multiplication:

$$
P^2 =\begin{bmatrix}
0 & \frac12 & \frac12 \\
\frac23 & 0 & \frac13 \\
\frac23 & \frac13 & 0
\end{bmatrix}\begin{bmatrix}
0 & \frac12 & \frac12 \\
\frac23 & 0 & \frac13 \\
\frac23 & \frac13 & 0
\end{bmatrix} = 
\begin{bmatrix}
\frac23 & \frac16 & \frac16 \\
\frac29 & \frac49 & \frac13 \\
\frac29 & \frac13 & \frac49
\end{bmatrix}
$$

For the desired probabilities we look at the third row. If the truck starts at the supermarket C at time 0, then at time 2 we expect the truck to be back at the supermarket C with probability $\frac49$, at supermarket B with probability $\frac13$ and at the processing facility A with probability $\frac29$.

# Problem B

## Part 1

$$
P^2 =\begin{bmatrix}
0.3 & 0.2 & 0.5 \\
0.5 & 0.1 & 0.4 \\
0.5 & 0.2 & 0.3
\end{bmatrix}\begin{bmatrix}
0.3 & 0.2 & 0.5 \\
0.5 & 0.1 & 0.4 \\
0.5 & 0.2 & 0.3
\end{bmatrix} = 
\begin{bmatrix}
0.44 & 0.18 & 0.38 \\
0.4 & 0.19 & 0.41 \\
0.4 & 0.18 & 0.42
\end{bmatrix}
$$

## Part 2

$P(X_2 = 0 | X_0 = 1) = P_{10}^2$, which we can find from the matrix $P^2$ that we just computed: $P(X_2 = 0 | X_0 = 1) = P_{10}^2 = 0.4$.

## Part 3

To find $P(X_2 = 1)$ we must multiply our matrix $P^2$ by the initial probabilities $\alpha^0$ to get $\alpha^2$. This works rather than multiplying the initial probailites by the matrix $P$ first and then multiplying by $P$ again because matrix vector multiplication is associative.

$$
\alpha^2 = \alpha^0P^2 =\begin{bmatrix}
0.5 & 0.5 & 0
\end{bmatrix}\begin{bmatrix}
0.44 & 0.18 & 0.38 \\
0.4 & 0.19 & 0.41 \\
0.4 & 0.18 & 0.42
\end{bmatrix} = 
\begin{bmatrix}
0.42 & 0.185 & 0.395
\end{bmatrix}
$$

So looking at $\alpha_1^2$ we know $P(X_2 = 1) = 0.185$.

## Part 3

To find $P(X_3 = 0)$ we follow the same forward probability calculation from the previous part.

$$
\alpha^3 = \alpha^2P =\begin{bmatrix}
0.42 & 0.185 & 0.395
\end{bmatrix}\begin{bmatrix}
0.3 & 0.2 & 0.5 \\
0.5 & 0.1 & 0.4 \\
0.5 & 0.2 & 0.3
\end{bmatrix} = 
\begin{bmatrix}
0.416 & 0.1815 & 0.4025
\end{bmatrix}
$$

So looking at $\alpha_0^3$ we know $P(X_3 = 0) = 0.416$.









